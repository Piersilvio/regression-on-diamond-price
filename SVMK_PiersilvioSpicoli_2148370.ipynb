{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikF8_zVmwNii"
   },
   "source": [
    "#  Regression on Diamonds Price Dataset with SVM\n",
    "\n",
    "The **Diamonds dataset** from Kaggle is a dataset containing information about the physical and pricing attributes of nearly 54,000 diamonds. The dataset is commonly employed in tasks like regression analysis, feature engineering, and exploratory data analysis.\n",
    "\n",
    "We will consider a **reduced version** of the dataset, containing 4000 samples, and without categorical features.\n",
    "\n",
    "### Key Features:\n",
    "- **Carat**: The weight of the diamond.\n",
    "- **Depth**: The total depth percentage (z / mean(x, y)).\n",
    "- **Table**: Width of the diamond's top as a percentage of its widest point.\n",
    "- **Price**: Price in US dollars.\n",
    "- **X, Y, Z**: Dimensions of the diamond in mm (length, width, depth).\n",
    "\n",
    "This dataset is useful for exploring relationships between physical attributes and pricing, and for building predictive models to estimate diamond prices based on their features.\n",
    "\n",
    "For more information see: https://www.kaggle.com/datasets/shivam2503/diamonds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hg5Gxo60wNin"
   },
   "source": [
    "# Overview\n",
    "\n",
    "In the notebook you will perform a complete pipeline of machine learning - regression task. First, you will:\n",
    "- split the data into training, validation, and test;\n",
    "- standardize the data.\n",
    "\n",
    "You will then be asked to learn various SVM models, in particular:\n",
    "- for each of the kernels *linear*, *poly*, *rbf*, and *sigmoid*, you will learn the best model, choosing among some fixed values of the considered hyperparameters. In particular, the choice of hyperparameters must be done with **5-fold cross-validation**, as we have seen in the labs.\n",
    "\n",
    "Then, from the models trained with the best hyperparameters selected as above, you will:\n",
    "- choose the best kernel, using a validation approach (not cross-validation), and\n",
    "- learn the best SVM model overall.\n",
    "\n",
    "Furthermore, you will then be asked to estimate the generalization error of the best SVM model you report.\n",
    "\n",
    "At the end, just for comparison, you will also be asked to learn a standard linear regression model (with squared loss), and estimate its generalization error.\n",
    "\n",
    "### IMPORTANT\n",
    "- Note that in each of the above steps you will have to choose the appropriate split of the data (see the first bullet point above);\n",
    "- The code should run without requiring modifications even if some best choice of parameters, changes; for example, you should not pass the best value of hyperparameters \"manually\" (i.e., passing the values as input parameters to the models). The only exception is in the TO DO titled 'ANSWER THE FOLLOWING'\n",
    "- $\\texttt{epsilon}$ parameter: For SVM, since the values to be predicted are all in the thousands of dollars, you will need to always set $\\texttt{epsilon} = 100$\n",
    "- Do not change the printing instructions (other than adding the correct variable name for your code), and do not add printing instructions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_r_viGpwNio"
   },
   "source": [
    "## TO DO - INSERT YOUR NUMERO DI MATRICOLA BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "i8DMVRcFwNip",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- put here your ID number (numero di matricola)\n",
    "numero_di_matricola = 2148370 # Spicoli Piersilvio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97fIw7F3wNiq"
   },
   "source": [
    "The following code loads all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1Fb8HFrywNir",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- import all packages needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsE_BSIbwNir",
    "tags": []
   },
   "source": [
    "The code below loads the data and remove samples with missing values. It also prints the number of samples and a brief description of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "RT2rfMf-wNis",
    "outputId": "db99ce5e-2eb2-486f-a4d9-616d654d19bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- load the data - do not change the path below!\n",
    "df = pd.read_csv('diamonds.csv', sep = ',')\n",
    "\n",
    "# -- remove the data samples with missing values (NaN)\n",
    "df = df.dropna()\n",
    "# -- let's drop the column containing the id of the data\n",
    "df = df.drop(columns=['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U33o6HkwwNis",
    "outputId": "9295fca7-8354-4548-dd49-41c240d749ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (4000, 7)\n",
      "             carat        depth       table         price            x  \\\n",
      "count  4000.000000  4000.000000  4000.00000   4000.000000  4000.000000   \n",
      "mean      0.797945    61.776925    57.44035   3920.239250     5.735810   \n",
      "std       0.462251     1.468899     2.26052   3935.292841     1.106897   \n",
      "min       0.210000    52.200000    52.00000    339.000000     0.000000   \n",
      "25%       0.400000    61.100000    56.00000    936.000000     4.720000   \n",
      "50%       0.710000    61.900000    57.00000   2468.000000     5.710000   \n",
      "75%       1.050000    62.500000    59.00000   5297.500000     6.550000   \n",
      "max       3.010000    70.600000    79.00000  18730.000000     9.100000   \n",
      "\n",
      "                 y            z  \n",
      "count  4000.000000  4000.000000  \n",
      "mean      5.736307     3.540002  \n",
      "std       1.099129     0.691834  \n",
      "min       0.000000     0.000000  \n",
      "25%       4.730000     2.910000  \n",
      "50%       5.730000     3.540000  \n",
      "75%       6.550000     4.040000  \n",
      "max       8.970000     5.670000  \n"
     ]
    }
   ],
   "source": [
    "print('Dataset shape:', df.shape)\n",
    "# -- description of dataset\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8abmrkXgwNit",
    "outputId": "8295b1dc-9a84-45d9-ecfc-7d45e74f1f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 samples of the dataset:\n",
      "\n",
      "    carat  depth  table  price     x     y     z\n",
      "0   0.33   61.7   55.0    564  4.43  4.46  2.74\n",
      "1   1.20   62.1   57.0   5914  6.78  6.71  4.19\n",
      "2   0.62   61.0   57.0   2562  5.51  5.54  3.37\n",
      "3   0.34   63.1   56.0    537  4.41  4.46  2.80\n",
      "4   1.20   62.5   55.0   5964  6.77  6.84  4.25\n"
     ]
    }
   ],
   "source": [
    "print('First 5 samples of the dataset:\\n\\n', df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weeyRmwWwNit"
   },
   "source": [
    "In the following cell, we convert our (pandas) dataframe into set X (containing our features) and the set Y (containing our target, i.e., the price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKmc72H0wNiu",
    "outputId": "43ff5613-c8bf-4571-b7a6-488aeec5a13e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 4000\n",
      "X shape:  (4000, 6)\n",
      "Y shape:  (4000,)\n"
     ]
    }
   ],
   "source": [
    "m = df.shape[0]\n",
    "\n",
    "# -- let's compute X and Y sets\n",
    "X = df.drop(columns=['price'], axis=1)\n",
    "Y = df['price']\n",
    "\n",
    "print(\"Total number of samples:\", m)\n",
    "\n",
    "X = X.values\n",
    "Y = Y.values\n",
    "\n",
    "# -- print shapes\n",
    "print('X shape: ', X.shape)\n",
    "print('Y shape: ', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofZ5-JpIwNiu",
    "tags": []
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOo6wVGOwNiu"
   },
   "source": [
    "## TO DO - SPLIT DATA INTO TRAINING, VALIDATION, AND TESTING, WITH THE FOLLOWING PERCENTAGES: 60%, 20%, 20%\n",
    "\n",
    "Use the $\\texttt{train\\_test\\_split}$ function from sklearn.model_selection to do it; in every call fix $\\texttt{random\\_state}$ to your numero_di_matricola.\n",
    "At the end, you should store the data in the following variables:\n",
    "- X_train, Y_train: training data;\n",
    "- X_val, Y_val: validation data;\n",
    "- X_train_val, Y_train_val: training and validation data;\n",
    "- X_test, Y_test: test data.\n",
    "\n",
    "The code then prints the number of samples in X_train, X_val, X_train_val, and X_test\n",
    "\n",
    "**IMPORTANT:**\n",
    "- first split the data into training+validation and test; the first part of the data in output from $\\texttt{train\\_test\\_split}$ must correspond to the training+validation;\n",
    "- then split training+validation into training and validation; the first part of the data in output from $\\texttt{train\\_test\\_split}$ must correspond to the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KMi3vXpwNiu",
    "outputId": "18f37d99-1628-4ec5-9d36-6d33e0978359",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 2400\n",
      "Validation size: 800\n",
      "Training and validation size: 3200\n",
      "Test size: 800\n"
     ]
    }
   ],
   "source": [
    "# -- split the data into training + validation and test\n",
    "\n",
    "m_test = int(X.shape[0] * 0.2)\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size = m_test, random_state = numero_di_matricola)\n",
    "\n",
    "# -- split the training + validation data into training and validation\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size = m_test, random_state = numero_di_matricola)\n",
    "\n",
    "print(\"Training size:\", X_train.shape[0])\n",
    "print(\"Validation size:\", X_val.shape[0])\n",
    "print(\"Training and validation size:\", X_train_val.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFakwfkzwNiu"
   },
   "source": [
    "## TO DO - STANDARDIZE THE DATA\n",
    "\n",
    "Standardize the data using the $\\texttt{preprocessing.StandardScaler}$ from scikit learn.\n",
    "\n",
    "If V is the name of the variable storing part of the data, the corresponding standardized version should be stored in V_scaled. For example, the scaled version of X_train should be stored in X_train_scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5CC4gP98wNiu",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled: mean =  [ 1.06563947e-14  6.73724994e-14 -4.63937916e-15 -9.38361657e-15\n",
      " -5.64099115e-15  2.22859347e-15]\n",
      "X_train_scaled: standard dev =  [1. 1. 1. 1. 1. 1.]\n",
      "X_train_val_scaled: mean =  [-0.0008297  -0.00664266  0.00203145 -0.000272   -0.00023556  0.00074503]\n",
      "X_train_val_scaled: standard dev =  [1.00314206 0.99307292 0.9999866  1.00024937 1.00007576 0.99504154]\n",
      "X_val_scaled: mean =  [-0.00331878 -0.02657065  0.00812579 -0.00108799 -0.00094226  0.00298013]\n",
      "X_val_scaled: standard dev =  [1.01250567 0.97172313 0.99992165 1.00099665 1.00030267 0.98001226]\n",
      "X_test_scaled: mean =  [-0.00773776 -0.01295922  0.0136107  -0.01789684 -0.01678134 -0.01149391]\n",
      "X_test_scaled: standard dev =  [1.03013979 1.02511242 1.00893552 1.03318451 1.03015919 1.007204  ]\n"
     ]
    }
   ],
   "source": [
    "s = preprocessing.StandardScaler()\n",
    "s.fit(X_train)\n",
    "X_train_scaled = s.transform(X_train)\n",
    "X_train_val_scaled = s.transform(X_train_val)\n",
    "X_val_scaled = s.transform(X_val)\n",
    "X_test_scaled = s.transform(X_test)\n",
    "\n",
    "print(\"X_train_scaled: mean = \", np.mean(X_train_scaled, axis=0))\n",
    "print(\"X_train_scaled: standard dev = \", np.std(X_train_scaled, axis=0))\n",
    "\n",
    "print(\"X_train_val_scaled: mean = \", np.mean(X_train_val_scaled, axis=0))\n",
    "print(\"X_train_val_scaled: standard dev = \", np.std(X_train_val_scaled, axis=0))\n",
    "\n",
    "print(\"X_val_scaled: mean = \", np.mean(X_val_scaled, axis=0))\n",
    "print(\"X_val_scaled: standard dev = \", np.std(X_val_scaled, axis=0))\n",
    "\n",
    "print(\"X_test_scaled: mean = \", np.mean(X_test_scaled, axis=0))\n",
    "print(\"X_test_scaled: standard dev = \", np.std(X_test_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6Zye8-wNiv",
    "tags": []
   },
   "source": [
    "# SVM models: learning the best model for each kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OP4kvBoJwNiv"
   },
   "source": [
    "The following function, i.e., $\\texttt{k\\_fold\\_cross\\_validation}$, will perform $k$-fold cross validation (with $k$ = 5 by default). Look carefully at the signature of the below function: you have in input some sets X and Y, the default number of folds, and a length-variable keyword argumens, with which the SVM model will be trained in the cross-validation phase. If you are not familiar with the notation, look at kwargs in Python documentation.\n",
    "\n",
    "In the first lines of the below function, the unpacked parameters (i.e., input parameter $\\texttt{param\\_grid}$) are converted into a python list by means of cartesian product. The resulting list (i.e., $\\texttt{param\\_list}$) will be the one for which you need to iterate over and perform $k$-fold cross-validation, using $\\texttt{KFold}$ object frmo scikit-learn.\n",
    "\n",
    "At the end, note that you need to return $\\texttt{best\\_param}$, that is the best set of parameters you found with the cross-validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zzkcmPVNwNiv"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def k_fold_cross_validation(X, Y, num_folds = 5, **param_grid):\n",
    "\n",
    "    # -- grid of hyperparams into list\n",
    "    param_keys = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    \n",
    "    # Generate Cartesian product of values\n",
    "    combinations = product(*param_values)\n",
    "    \n",
    "    # Create a list of dictionaries from combinations\n",
    "    param_list = [dict(zip(param_keys, combination)) for combination in combinations]\n",
    "\n",
    "    # -- TODO\n",
    "    kf = KFold(n_splits = num_folds)\n",
    "    err_train_kfold = np.zeros(len(param_list))\n",
    "    err_val_kfold = np.zeros(len(param_list))\n",
    "    \n",
    "    for i, param in enumerate(param_list):\n",
    "        \n",
    "        model = svm.SVR(**param)\n",
    "        \n",
    "        for train_index, validation_index in kf.split(X):\n",
    "            X_train_kfold, X_val_kfold = X[train_index], X[validation_index]\n",
    "            Y_train_kfold, Y_val_kfold = Y[train_index], Y[validation_index]\n",
    "            \n",
    "            scaler_kfold = preprocessing.StandardScaler().fit(X_train_kfold)\n",
    "            X_train_kfold_scaled = scaler_kfold.transform(X_train_kfold)\n",
    "            X_val_kfold_scaled = scaler_kfold.transform(X_val_kfold)\n",
    "            \n",
    "            model.fit(X_train_kfold_scaled, Y_train_kfold)\n",
    "            Y_pred_train = model.predict(X_train_kfold_scaled)\n",
    "            Y_pred_val = model.predict(X_val_kfold_scaled)\n",
    "\n",
    "            err_train_kfold[i] += (1 - model.score(X_train_kfold_scaled, Y_train_kfold))\n",
    "            err_val_kfold[i] += (1 - model.score(X_val_kfold_scaled, Y_val_kfold))\n",
    "\n",
    "    err_train_kfold /= num_folds\n",
    "    err_val_kfold /= num_folds\n",
    "    best_param = param_list[np.argmin(err_val_kfold)]\n",
    "    \n",
    "    return best_param\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9J2nywBwNiv"
   },
   "source": [
    "## TO DO - CHOOSE THE BEST HYPERPARAMETERS FOR LINEAR KERNEL\n",
    "\n",
    "For the SVM, consider $\\texttt{svm.SVR}$ class. We will begin by training the SVM with linear kernel. For the latter, consider the following hyperparameters and their values:\n",
    "\n",
    "- $C: [0.1, 1, 10, 100, 1000]$\n",
    "\n",
    "Remember that both the $\\texttt{kernel}$ type and the value of $\\texttt{epsilon}$ are considered as parameters to pass to the above method. Leave all other input parameters to default.\n",
    "\n",
    "Find the best value of the hyperparameters using 5-fold cross validation. Use the function defined above to perform the cross-validation.\n",
    "\n",
    "Print the best value of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xrrdvu_ewNiv",
    "outputId": "bb89d6a1-9b05-47b3-c32b-4a52cfe35984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear SVM:\n",
      "Best value for hyperparameters:  {'C': 1000, 'epsilon': 100, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLinear SVM:\")\n",
    "\n",
    "linear_param_grid = {\n",
    "    \"C\": [0.1, 1, 10, 100, 1000],\n",
    "    \"epsilon\": [100],\n",
    "    \"kernel\": [\"linear\"]\n",
    "}\n",
    "\n",
    "best_param_linear = k_fold_cross_validation(X_train_scaled, Y_train, num_folds=5, **linear_param_grid)\n",
    "\n",
    "print(\"Best value for hyperparameters: \", best_param_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KORMxmuwNiv"
   },
   "source": [
    "## TO DO - LEARN A MODEL WITH LINEAR KERNEL AND BEST CHOICE OF HYPERPARAMETERS\n",
    "\n",
    "This model will be compared with the best models with other kernels using validation (not cross validation).\n",
    "\n",
    "DO NOT PASS PARAMETERS BY HARD-CODING THEM IN THE CODE.\n",
    "\n",
    "Print the **training score** (that is, $R^2$ coefficient) of the best model, trained with the best parameter find from the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe4pW7-3wNiv",
    "outputId": "11036381-7ba5-475b-ca8d-fb2e31fa01c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.8420874176798536\n"
     ]
    }
   ],
   "source": [
    "linear_model = svm.SVR(**best_param_linear)\n",
    "linear_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "print(\"Training score:\", linear_model.score(X_train_scaled, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmQqtqlrwNiv"
   },
   "source": [
    "## TO DO - CHOOSE THE BEST HYPERPARAMETERS FOR POLY KERNEL\n",
    "\n",
    "Now, let's consider $\\texttt{svm.SVR}$ with polynomial kernel. Consider the following hyperparameters and their values:\n",
    "- $C: [0.1, 1, 10, 100, 1000]$\n",
    "- $degree: [2, 3, 4]$\n",
    "\n",
    "Leave all other input parameters to default.\n",
    "\n",
    "Find the best value of the hyperparameters using 5-fold cross validation. Use the function defined above to perform the cross-validation.\n",
    "\n",
    "Print the best value of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__GfD2qJwNiw",
    "outputId": "12784769-b138-4961-eff1-cab434c78078",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Poly SVM\n",
      "Best value for hyperparameters:  {'C': 10, 'kernel': 'poly', 'epsilon': 100, 'degree': 3}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPoly SVM\")\n",
    "\n",
    "param_grid_poly = {\n",
    "    \"C\": [0.1, 1, 10, 100, 1000],\n",
    "    \"kernel\": [\"poly\"],\n",
    "    \"epsilon\": [100],\n",
    "    \"degree\": [2, 3, 4]\n",
    "}\n",
    "\n",
    "best_param_poly = k_fold_cross_validation(X_train_scaled, Y_train, num_folds=5, **param_grid_poly)\n",
    "\n",
    "print(\"Best value for hyperparameters: \", best_param_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DykjwP8fwNiw",
    "tags": []
   },
   "source": [
    "## TO DO - LEARN A MODEL WITH POLY KERNEL AND BEST CHOICE OF HYPERPARAMETERS\n",
    "\n",
    "This model will be compared with the best models with other kernels using validation (not cross validation).\n",
    "\n",
    "DO NOT PASS PARAMETERS BY HARD-CODING THEM IN THE CODE.\n",
    "\n",
    "Print the **training score** (that is, $R^2$ coefficient) of the best model, trained with the best parameter find from the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s12MCenJwNiw",
    "outputId": "ba1d24c3-5a30-470b-8695-e104a7c04ca3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.47421265911874444\n"
     ]
    }
   ],
   "source": [
    "poly_model = svm.SVR(**best_param_poly)\n",
    "poly_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "print(\"Training score:\", poly_model.score(X_train_scaled, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLKX5fHewNiw"
   },
   "source": [
    "## TO DO - CHOOSE THE BEST HYPERPARAMETERS FOR RBF KERNEL\n",
    "\n",
    "Consider $\\texttt{svm.SVR}$ with RBF kernel. Consider the following hyperparameters and their values:\n",
    "- $C: [0.1, 1, 10, 100, 1000]$\n",
    "- $gamma: [0.01, 0.03, 0.04, 0.05]$\n",
    "\n",
    "Leave all other input parameters to default.\n",
    "\n",
    "Find the best value of the hyperparameters using 5-fold cross validation. Use the function defined above to perform the cross-validation.\n",
    "\n",
    "Print the best value of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5JE-N_9wNiw",
    "outputId": "82be010d-5bdb-40ad-df87-679239360636",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RBF SVM\n",
      "Best value for hyperparameters:  {'C': 1000, 'gamma': 0.04, 'epsilon': 100, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRBF SVM\")\n",
    "\n",
    "param_grid_RBF = {\n",
    "    \"C\": [0.1, 1, 10, 100, 1000],\n",
    "    \"gamma\": [0.01, 0.03, 0.04, 0.05],\n",
    "    \"epsilon\": [100],\n",
    "    \"kernel\": [\"rbf\"]\n",
    "}\n",
    "\n",
    "best_param_RBF = k_fold_cross_validation(X_train_scaled, Y_train, num_folds=5, **param_grid_RBF)\n",
    "\n",
    "print(\"Best value for hyperparameters: \", best_param_RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGCzoRxiwNiw"
   },
   "source": [
    "## TO DO - LEARN A MODEL WITH RBF KERNEL AND BEST CHOICE OF HYPERPARAMETERS\n",
    "\n",
    "This model will be compared with the best models with other kernels using validation (not cross validation).\n",
    "\n",
    "DO NOT PASS PARAMETERS BY HARD-CODING THEM IN THE CODE.\n",
    "\n",
    "Print the **training score** (that is, $R^2$ coefficient) of the best model, trained with the best parameter find from the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXOS5B7UwNiw",
    "outputId": "4a0a4735-c8e8-4465-83df-267e344c8fe0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.860381286714856\n"
     ]
    }
   ],
   "source": [
    "RBF_model = svm.SVR(**best_param_RBF)\n",
    "RBF_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "print(\"Training score:\", RBF_model.score(X_train_scaled, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8UtDOhNwNiw",
    "tags": []
   },
   "source": [
    "## TO DO - CHOOSE THE BEST HYPERPARAMETERS FOR SIGMOID KERNEL\n",
    "\n",
    "Consider $\\texttt{svm.SVR}$ with sigmoid kernel. Consider the following hyperparameters and their values:\n",
    "- $C: [0.1, 1, 10, 100, 1000]$\n",
    "- $gamma: [0.01, 0.05, 0.1]$\n",
    "- $coef0: [0, 1]$\n",
    "\n",
    "Leave all other input parameters to default.\n",
    "\n",
    "Find the best value of the hyperparameters using 5-fold cross validation. Use the function defined above to perform the cross-validation.\n",
    "\n",
    "Print the best value of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uFmykTmwNiw",
    "outputId": "9dda95e0-01f8-4205-f9af-f7cd612e817b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sigmoid SVM\n",
      "Best value for hyperparameters:  {'C': 1000, 'gamma': 0.01, 'kernel': 'sigmoid', 'epsilon': 100, 'coef0': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSigmoid SVM\")\n",
    "\n",
    "param_grid_sig = {\n",
    "    \"C\": [0.1, 1, 10, 100, 1000],\n",
    "    \"gamma\": [0.01, 0.05, 0.1],\n",
    "    \"kernel\": [\"sigmoid\"],\n",
    "    \"epsilon\": [100],\n",
    "    \"coef0\": [0, 1]\n",
    "}\n",
    "\n",
    "best_param_sig = k_fold_cross_validation(X_train_scaled, Y_train, num_folds=5, **param_grid_sig)\n",
    "\n",
    "print(\"Best value for hyperparameters: \", best_param_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs0_yd3owNix",
    "tags": []
   },
   "source": [
    "## TO DO - LEARN A MODEL WITH SIGMOID KERNEL AND BEST CHOICE OF HYPERPARAMETERS\n",
    "\n",
    "This model will be compared with the best models with other kernels using validation (not cross validation).\n",
    "\n",
    "DO NOT PASS PARAMETERS BY HARD-CODING THEM IN THE CODE.\n",
    "\n",
    "Print the **training score** (that is, $R^2$ coefficient) of the best model, trained with the best parameter find from the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-H4FlTnbwNix",
    "outputId": "2ef7de44-411b-44c9-991e-8acacfd3de98",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.7783920869877037\n"
     ]
    }
   ],
   "source": [
    "sig_model = svm.SVR(**best_param_sig)\n",
    "sig_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "print(\"Training score:\", sig_model.score(X_train_scaled, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-JFbThkwNix",
    "tags": []
   },
   "source": [
    "## TO DO - USE VALIDATION TO CHOOSE THE BEST MODEL AMONG THE ONES LEARNED FOR THE VARIOUS KERNELS\n",
    "\n",
    "Use validation to choose the best model among the four ones (one for each kernel) you have learned above.\n",
    "\n",
    "Print, following exactly the order described here, with 1 value for each line:\n",
    "- the validation score of SVM with linear kernel (the template below does not include such print)\n",
    "- the validation score of SVM with polynomial kernel (the template below does not include such print)\n",
    "- the validation score of SVM with rbf kernel (the template below does not include such print)\n",
    "- the validation score of SVM with sigmoid kernel (the template below does not include such print)\n",
    "- the best kernel (e.g., sigmoid)\n",
    "- the validation score of the best kernel\n",
    "\n",
    "For the first 4 prints, use the format: \"*kernel* validation score: \". For example, for linear kernel \"linear validation score: \", for rbf \"rbf validation score: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnlSPDEwwNix",
    "outputId": "cafa4754-f625-448a-fdbc-cb90e2b37bec",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION TO CHOOSE SVM KERNEL:\n",
      "linear validation score: 0.8282\n",
      "polynomial validation score: 0.4519\n",
      "rbf validation score: 0.8651\n",
      "sigmoid validation score: 0.7981\n",
      "\n",
      "---\n",
      "Best kernel: rbf\n",
      "Best parameters for the best kernel: {'C': 1000, 'gamma': 0.04}\n",
      "Validation score of best kernel: 0.8651\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVALIDATION TO CHOOSE SVM KERNEL:\")\n",
    "\n",
    "validation_scores = {\n",
    "    \"linear\": linear_model.score(X_val_scaled, Y_val),\n",
    "    \"polynomial\": poly_model.score(X_val_scaled, Y_val),\n",
    "    \"rbf\": RBF_model.score(X_val_scaled, Y_val),\n",
    "    \"sigmoid\": sig_model.score(X_val_scaled, Y_val)\n",
    "}\n",
    "\n",
    "main_params_keys = {\n",
    "    \"linear\": [\"C\"],\n",
    "    \"polynomial\": [\"C\", \"degree\", \"gamma\"],\n",
    "    \"rbf\": [\"C\", \"gamma\"],\n",
    "    \"sigmoid\": [\"C\", \"gamma\"]\n",
    "}\n",
    "\n",
    "grid_params = {\n",
    "    \"linear\": {key: linear_model.get_params()[key] for key in main_params_keys[\"linear\"]},\n",
    "    \"polynomial\": {key: poly_model.get_params()[key] for key in main_params_keys[\"polynomial\"]},\n",
    "    \"rbf\": {key: RBF_model.get_params()[key] for key in main_params_keys[\"rbf\"]},\n",
    "    \"sigmoid\": {key: sig_model.get_params()[key] for key in main_params_keys[\"sigmoid\"]}\n",
    "}\n",
    "\n",
    "best_kernel = max(validation_scores, key=validation_scores.get)\n",
    "best_score = validation_scores[best_kernel]\n",
    "best_params = grid_params[best_kernel]\n",
    "\n",
    "for kernel, score in validation_scores.items():\n",
    "    print(f\"{kernel} validation score: {score:.4f}\")\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(f\"Best kernel: {best_kernel}\")\n",
    "print(f\"Best parameters for the best kernel: {best_params}\")\n",
    "print(f\"Validation score of best kernel: {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0NObJ-PwNix"
   },
   "source": [
    "## TO DO - LEARN THE FINAL MODEL FOR WHICH YOU WANT TO ESTIMATE THE GENERALIZATION SCORE\n",
    "\n",
    "Learn the final model (i.e., the one you would use to make predictions about future data).\n",
    "\n",
    "Print the **final model hyperparameters** and the **score** of the model on the data used to learn it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dz1Lwlg-wNi1",
    "outputId": "3d6e312e-1ad2-4b8b-dfd3-5e45b9e1e346",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEST MODEL:\n",
      "Best model hyperparameters: {'C': 1000, 'gamma': 0.04}\n",
      "Score of the best model on the data used to learn it:  0.8610769454259236\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBEST MODEL:\")\n",
    "\n",
    "final_model = svm.SVR(**best_params)\n",
    "final_model.fit(X_train_val_scaled, Y_train_val)\n",
    "\n",
    "train_score = final_model.score(X_train_val_scaled, Y_train_val)\n",
    "\n",
    "print(\"Best model hyperparameters:\", best_params)\n",
    "print(\"Score of the best model on the data used to learn it: \", train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbVgaDWXwNi2",
    "tags": []
   },
   "source": [
    "## TO DO - PRINT THE ESTIMATE  OF THE GENERALIZATION SCORE FOR THE FINAL MODEL\n",
    "\n",
    "Print the estimate of the generalization **score** for the final model. The generalization \"score\" is the score computed on the data used to estimate the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skvbHFUxwNi2",
    "outputId": "63a368d0-7810-4b3c-9ea8-80db9ca68943",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GENERALIZATION SCORE BEST MODEL:\n",
      "Best model hyperparameters: {'C': 1000, 'gamma': 0.04}\n",
      "Generalization score (R²) of the best model on the test data: 0.8839\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGENERALIZATION SCORE BEST MODEL:\")\n",
    "\n",
    "final_model = svm.SVR(**best_params)  \n",
    "final_model.fit(X_train_val_scaled, Y_train_val)  \n",
    "\n",
    "test_score = final_model.score(X_test_scaled, Y_test) \n",
    "\n",
    "print(\"Best model hyperparameters:\", best_params)\n",
    "print(f\"Generalization score (R²) of the best model on the test data: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVPCWbp6wNi2",
    "tags": []
   },
   "source": [
    "## TO DO - ANSWER THE FOLLOWING\n",
    "\n",
    "Print the **training score** (score on data used to train the model) and the **generalization score** (score on data used to assess generalization) of the final SVM model THAT YOU OBTAIN WHEN YOU RUN THE CODE, one per line, printing the smallest one first.\n",
    "\n",
    "NOTE: THE VALUES HERE SHOULD BE HARDCODED.\n",
    "\n",
    "Print you answer (YES/NO) to the following question: does the relation (i.e., smaller, larger) between the training score and the generalization score agree with the theory?\n",
    "\n",
    "Print your motivation for the YES/NO answer above, using at most 500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "AQdkt141wNi2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANSWER\n",
      "Training score:  0.8610769454259236\n",
      "Generalization score:  0.8839112859838582\n",
      "According to the theory, the obtained results are not consistent because, conceptually, the error on the training data should be slightly higher than the generalization error, and this behavior is due to excessive regularization in the training phase.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nANSWER\")\n",
    "\n",
    "# -- note that you may have to invert the order of the following 2 lines, print the smallest 1 first\n",
    "print(\"Training score: \", train_score)\n",
    "print(\"Generalization score: \", test_score)\n",
    "\n",
    "\n",
    "# -- the following is a string with you answer\n",
    "motivation = \"According to the theory, the obtained results are not consistent because, conceptually, the error on the training data should be slightly higher than the generalization error, and this behavior is due to excessive regularization in the training phase.\"\n",
    "\n",
    "print(motivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjIJf3s3wNi2",
    "tags": []
   },
   "source": [
    "## TO DO: LEARN A STANDARD LINEAR MODEL\n",
    "Learn a standard linear model using scikit learn.\n",
    "\n",
    "Print the **score** of the model on the data used to learn it.\n",
    "\n",
    "Print the **generalization score** of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "GAVYBA3wwNi2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR MODEL\n",
      "Score of LR model on data used to learn it:  0.8488909277502004\n",
      "Generalization score of LR model:  0.8651633433441661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"\\nLR MODEL\")\n",
    "\n",
    "linear_regression_model = LinearRegression()\n",
    "linear_regression_model.fit(X_train_val_scaled, Y_train_val)\n",
    "\n",
    "print(\"Score of LR model on data used to learn it: \", linear_regression_model.score(X_train_val_scaled, Y_train_val))\n",
    "print(\"Generalization score of LR model: \", linear_regression_model.score(X_test_scaled, Y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
